---
Связи: 
tags: 
links:
---
Некоторые задачи требуют большого контекста.
Сложно ванильного [[Attention (DL)]] является O(n^2), растет квадратично, со, длинные последовательности - узкое место [[Transformer Architecture]]

Задачи:
- Writing books
- Storytelling
- Building intelligent agents for conversations like chatbots.