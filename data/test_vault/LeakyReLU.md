---
tags:
  - "#DL"
links:
---
[[Activation function]], которая призвана починить минусы [[Rectified Linear Unit (ReLU)]], когда там зануляется градиент (если он около или меньше 0)
Спойлер - работать будет не всегда, лучше чинить это в архитектуре
![[Pasted image 20241130102345.png]]