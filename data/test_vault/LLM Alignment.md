---
tags: 
links:
---
Посути говорим о том-же, что и [[LLM Finetuning]]
Базируется на лекции Яндекса https://youtu.be/f5JFXvX7FLE?list=PL6Wui14DvQPzMqtOOnfL00ZQ61JcN8V87

- [[#Abstract|Abstract]]
- [[#Supervised Finetuning|Supervised Finetuning]]
- [[#Evaluation of LLM|Evaluation of LLM]]
- [[#Асессоры и дообучение|Асессоры и дообучение]]
	- [[#Асессоры и дообучение#Дистиляция ассесоров|Дистиляция ассесоров]]
- [[#RL|RL]]
	- [[#RL#Cross-entropy method in LLM|Cross-entropy method in LLM]]
- [[#Policy gradinents method|Policy gradinents method]]


## Abstract
Есть какая-то [[LLM  - Large Language Models|LLM]], выплевывающая наиболее вероятный некст токен.
Хотим, чтоб она не только выплевывала некст токен, но отвечала на вопросы и помогала.
И была классной по качеству (а это надо оценивать как-то)


## Supervised Finetuning
- Собрать всевозможные запросы юзеров - инструкции
	- Найти правильные ответы
	- Дообучить на пары инструкция-ответ
- Дает 80% качества за 20% времени
- Ботлнек - надо много данных

## Evaluation of LLM
- Harmless, Helpful, Honest - paper от Antropic https://arxiv.org/abs/2112.00861
- RLFH - Reinforcment Learning from Human Feedback

![[Pasted image 20231212140949.png]]


## Асессоры и дообучение
- Отдаешь ассесорам ответы модели и получаешь клики, какой вариант лучше. Нюансы
	- Порядок важен. Люди чаще лайкают первый ответ
	- Honey Pot. Ассесоры ленятся, нужно проверять, что они живы
	- Транзитивность. if a>b and b>c: a>c? Обычно то да, но не всегда

### Дистиляция ассесоров
- Учим модель награды, которая умеет оценивать базовую модель рис 1
- Кейсы использования модели-оценщика
	- Ранний останов обучения с учителем рис 2 - там тоже может быть переобучение
	- Выбор модельки\гиперпараметров
	- Ранжирование ответов
	- Всегда нужно делать это разумно, а не слепо верить
- Обучение с подкреплением
![[Pasted image 20231212142147.png]]![[Pasted image 20231212142648.png]]![[Pasted image 20231212143339.png]]

## RL
[[Reinforcment Learning]]
Есть робот (агент), взаимодействующий со средой. Агент наблюдает за средой и выбирает действие (в соответствии со своей политикой).
Агент направляет действия в среду, среда изменяется и дает фидбек агенту(награду)
- Мы пытаемся максимизировать награду

Как это применимо в LLM?
- Пусть наше взаимодействие одношаговое
- Агент смотрит на среду. Делает какое-то действие. Среда на это отвечает наградой
	- Хотим максимизировать по политикам (меняя поведение агента)
	- Хотим выбирать те политики, которые генерируют овтеты, приводящие к макс награде
![[Pasted image 20231212143438.png]]![[Pasted image 20231212143922.png]]


### Cross-entropy method in LLM
- Как работает Кросс-энтропийный метод можно посмотреть на странице [[Reinforcment Learning|RL]]
- У нас есть LLMка и Reward model, и некоторый датасет инструкций
	- Прогоняем инструкции через LLMку, получаем N(пусть 10) ответов
	- Теперь нужно поправить веса LLMки так, чтоб вероятность генерации ответов соответствовала ранжированию
	- Ну, для этого можно просто использовать метод обучения с учителем
- Также называется RL для бедных, мы сводим RL задачу к обучению с учителем
![[Pasted image 20231213194150.png]]


### Policy gradinents method
Идти читать в Reinforcment Learning про сам метод


### Tricks and problems
Трюки и проблемы
- Политики вырождаются в детерменированные рис 1
	- Решение - добавлять энтропию к награде!
-  Доменный сдвиг - рис 2
	- если метрику доводим до конца, она перестает быть хорошей
	- борьба - давайте не будем давать модели далеко - рис 3
	- Например по [[Kullback–Leibler divergence]]
- Борьба с помощью ансамбля моделей наград рис 4

![[Pasted image 20231213200111.png]]
![[Pasted image 20231213204253.png]]![[Pasted image 20231213204335.png]]![[Pasted image 20231213205151.png]]