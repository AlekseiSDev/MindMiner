---
Связи: 
tags:
  - Paper
  - InterviewPapers
links:
---
Базовая статья NLP, положившая начало [[Transformer Architecture]]

- [[#Basics|Basics]]
	- [[#Basics#Feed Forward|Feed Forward]]
- [[#Self-attention|Self-attention]]


## Basics
Предложена [[Decoder (nn)]]- [[Encoder (nn)]] архитектура, где все слои посути идентичные
- Каждый слой энкодера и декодера это FFNN и Self-attention, рис 2
- Каждый принимает и отдает эмбеды размером D
![[Pasted image 20231206113629.png]]
![[Pasted image 20231206113644.png]]


### Feed Forward
- Отвечает за часть, называем Computational, дополнительные преобразования и усложнения, позволяющие сетки перформить
- 2 линейных слоя, первый расширяет размерность, второй сжимает обратно к входу
	- Посередине нелинейность, исторически ReLU, сейчас чето лучше бывает робит
	- В оригинале D_m = 768, D_ff = 3072
	- Обычно соблюдают примерно эту пропорцию, расширяют раза в 4, хотя PaLM -не
- В них большинство весов (2\3 или 3\4) Трансформера, и наверное тут должно храниться большинство знаний
- И это действительно так, и пейпер arxiv.org/abs/2012.14913 это показывает
- Каждый  [[Dense(Linear or Fully-connected or FC) layer|линеный слой]] это на самом деле матричка размером in_dim, out_dim
	- Первый наш слой будет D_m, D_ff; а второй - D_ff, D_m
	- между нелинейность
	- Представим, что мы прогнали аутпут атеншена через первый слой и релу и получился вектор, где все 0 и где-то чиселка 7
	- Посути, это вес, с которым мы должны взять исходную информацию из входной матрички? Во всяком случае говорится в пейепере 
	- Посути, вторая матрица хранит знания, а 1я, первый линейный слой, он указывает их веса
	- Внимательно разобрать [[Transformer Feed-Forward Layers Are Key-Value Memories]]
	- Чем выше уровень энкодера, тем более комплексные паттерны находят эти блоки

![[Pasted image 20231206113727.png]]![[Pasted image 20231206113735.png]]![[Pasted image 20231206115141.png]]



## Self-attention
[[Self-Attention]]