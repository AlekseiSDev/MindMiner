---
Связи: 
tags:
  - "#DL #ML #NLP"
---
Задача [[machine learning]], для которой часто используется [[Deep Learning]]
Суть задачи перевести текст с помощью машины (кэп)

- [[#Подход через векторные трансформации|Подход через векторные трансформации]]
	- [[#Подход через векторные трансформации#Поиск преобразования R, X@R -> Y|Поиск преобразования R, X@R -> Y]]
- [[#Neural Machine Translation -  NMT|Neural Machine Translation -  NMT]]
	- [[#Neural Machine Translation -  NMT#Abstract|Abstract]]
	- [[#Neural Machine Translation -  NMT#NMT Seq2seq architecture|NMT Seq2seq architecture]]

## Подход через векторные трансформации
- Мы можем представить корпусы обоих языков как набор векторов
- Значит мы можем найти такую трансформацию, которую взяв вектор 1го языка приведет его к вектору другого(но это не точно)
	- Назовем это преобразование R и будем его искать
	- С гипотезой, что оно обобщаемо - т.е. мы найдем его для небольшого сабсета языков, и оно будет работать для фулл языков
- Ищем наиболее близкий вектор через [[K-nearest neighbors]]

![[Pasted image 20230901100227.png]]
![[Pasted image 20230901100530.png]]


### Поиск преобразования R, X@R -> Y
- Нужно минимизировать разницы между XR - Y
- Можно минимизировать их [[Frobenius norm]]
	- А лучше норму фробениуса в квадрате - так проще


![[Pasted image 20230901100923.png]]![[Pasted image 20230901100943.png]]
![[Pasted image 20230901101650.png]]![[Pasted image 20230901102447.png]]


## Neural Machine Translation -  NMT

### Abstract
Базируется на [[Seq2seq]] подходе, а современные решения и вовсе на [[Transformer Architecture]]
- task
- setup
- tokens example eng (1)
![[Pasted image 20230926190246.png]]![[Pasted image 20230926190323.png]]![[Pasted image 20230926190347.png]]![[Pasted image 20230926190418.png]]


### NMT Seq2seq architecture
- Выглядит как-то так (рис 1)
- Декодер для NMT сложно трейнить и используется претрейнд декодер, который треним [[Teacher forcing]]
- Модель шаг за шагом
	- Создать копии входных данных
	- Отдать часть инпута в Encoder
	- Отдать таргеты в обучение Pre-attention Decoder
		- Они с энкодером учатся параллельно и посути независимы пока
	- В декодере предварительного внимания вы сдвигаете каждую последовательность вправо и добавляете маркер начала предложения. В кодере и декодере предварительного внимания входные и целевые данные проходят через уровень ембедов, прежде чем поступать в LSTM.
![[Pasted image 20230926193244.png]]![[Pasted image 20230926193517.png]]
![[Pasted image 20230926194825.png]]
![[Pasted image 20230926194806.png]]![[Pasted image 20230927104326.png]]


### Sampling and Decoding
- Каждое наше слово зависит во многом от того, что вышло предыдущим аутпутом. И тут есть разные хитрости, как лучше генерить все предложение

#### Greedy decoding
- Тупо наиболее вероятное на каждом шагу
- Хо
- Ограничениия - обычно не оч на длинных предложениях, 
![[Pasted image 20230927115038.png]]
![[Pasted image 20230927115227.png]]


#### Random sampling
- Выбираем рандомное слово
	- или каждое слово честно реализуя его вероятности?)
![[Pasted image 20230927115424.png]]

##### Temperature in sampling
- Определеяет насколько рандомные предикты выдаются
- Измереяется  0 до 1
- Чем ниже, тем более консервативно будет вести себя сеточка
![[Pasted image 20230927120603.png]]



