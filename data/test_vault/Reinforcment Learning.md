---
tags: 
links:
---
Большой раздел [[machine learning|ML]], занимающийся обучением агентов, не имеющих учителя, а получающих фидбек от внешней среды и своих взаимодействий с ней.
Годный конспект-книга https://arxiv.org/abs/2201.09746

- [[#Abstract|Abstract]]
- [[#Почему с подкреплением|Почему с подкреплением]]
- [[#Cross-entropy method|Cross-entropy method]]


## Abstract
Есть робот (агент), взаимодействующий со средой. Агент наблюдает за средой и выбирает действие (в соответствии со своей политикой).
Агент направляет действия в среду, среда изменяется и дает фидбек агенту(награду)
![[Pasted image 20231212143438.png]]


## Почему с подкреплением
- Собачки учатся как и роботы. Рис 1.
	- Хозяин выступает с помощью среды, и спускать награды
	- А также дает инструкции среды
	- Важный момент - мы не говорим, какие мышцы ей следует напрячь чтоб сесть - мы говорим команду на нашем языке, а когда собака случайно сделает то, что мы хотим - даем ей награду
- Также учатся агенты - взаимодействуют со средой, пока не получат награду

![[Pasted image 20231213192343.png]]



## Cross-entropy method
Простой и рабочий метод, посути эволюционный алгоритм оптимизации
- Представим у нас есть функция от y(x0, x1) , рис 1, звездочка - оптимум
- Давайте инициализируем какое-то распределение: пусть Гаусово рис 2
- Из него случайно породим точки - это наше поколение 0 рис 3
- Отфильтруем точки с наибольшим значением к нашему оптимуму рис 4
	- элитные точки
- Хорошо, надо сдвинуть распределение точки. Давайте используем [[Maximum likelihood estimation]]
- Находим новую гаусиану рис 5
- Максимизировать правдоподобие = минимизировать [[Cross-entropy loss]] - отсюда название
- Продолжаем двигаться к нужной точке до сходимости
- Это простой метод безградиентной оптмизации


![[Pasted image 20231213192521.png]]![[Pasted image 20231213192611.png]]![[Pasted image 20231213192650.png]]![[Pasted image 20231213192938.png]]![[Pasted image 20231213193359.png]]![[Pasted image 20231213194011.png]]




## Policy gradient
Градиент по политике
Хотим максимизировать суммарную награду
Политика - нейронная сеть, параметризуется параметрами тетта
Ну, мы можем пойти по градиенту 
- Нюанс: мы берем град по тетта, а тетта сидит в мат-ожидание
- Если подсчитать град по матожиданию - получится интегральчик рис 2
- Откуда он? Из матожидания, это же в среднем и есть интеграл
	- А как мы раньше их обходили - ну, для матожиданий есть трюк с Монте-Карло, мы можем набрать генераций и усреднить, и получить чиселку матожидания. Но ща оно перестало быть чисто МО, стало интегралом
	- Но можно обойти через log derivative trick!
	- Подставим в формулку, получим
- Как в итоге выглядит модель: рис 4
	- Максимизация через суррогатную функцию
	- Для каждого шага перегенерируем ответ - это важно, чтоб град был правильный!

![[Pasted image 20231213194848.png]]![[Pasted image 20231213195130.png]]![[Pasted image 20231213195438.png]]![[Pasted image 20231213195549.png]]


### Policy gradients trick
- Высокая градиента
- Перегенерирование на каждом шагу
	- Тут прям набор трюков, вместе соединены в методе proximal policy optimization

![[Pasted image 20231213195735.png]]