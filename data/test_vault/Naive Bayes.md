---
Связи:
Теги:
---
Алгоритм машинного обучения, основанный на [[Bayes' rule]]

- [[#Краткая суть|Краткая суть]]
- [[#Training|Training]]
- [[#Testing Naive Bayes|Testing Naive Bayes]]
- [[#Applications of Naive Bayes|Applications of Naive Bayes]]
- [[#Assumptions udnerlying the method|Assumptions udnerlying the method]]
- [[#Some errors and methods to fix them|Some errors and methods to fix them]]


## Краткая суть
1) составить таблицу значений фичей при class 1 и class 2
2) На инференсе для каждого твита брать произведение (от слов) деления вероятностностей


![[Pasted image 20230829234125.png]]
![[Pasted image 20230829235248.png]]


## Training
0) Собрать и разметить датку
1) Препроцессинг
2) Составить таблицу частоты
3) Получить вероятность каждого слова в каждом классе по Laplacian Smoothing
4) Найти лямбду 
5) Найти logprior - и добавить его к итоговому значению вероятности - поправка на дисбаланс классов

![[Pasted image 20230829234345.png]]![[Pasted image 20230829234420.png]]
![[Pasted image 20230829234514.png]]
![[Pasted image 20230829234630.png]]
![[Pasted image 20230829234642.png]]

## Testing Naive Bayes
- Все как обычно - делаем предикты
![[Pasted image 20230830083740.png]]
![[Pasted image 20230830083752.png]]
![[Pasted image 20230830083819.png]]

## Applications of Naive Bayes

Наивный Байес может использоваться в кучи задач классификации, т.к. простой и легко интепретируемый. Обычно он берется как бейзлайн
0) Наша классификация позитива/негатива
1) По словам определить, кому из авторов принадлежит книга
2) Спам фильтр
3) Релевантность дока - уже интереснее. У нас есть qyuery и мы возращаем по нему doc, с помощью формулы Байеса можно прикинуть, нужный ли док вернули. Наверное сложно учить? 
Вот тут о применениях много https://nlp.stanford.edu/IR-book/pdf/13bayes.pdf
![[Pasted image 20230830083948.png]]
![[Pasted image 20230830084028.png]]
![[Pasted image 20230830084610.png]]

## Assumptions udnerlying the method
Метод назван наивным, т.к. держит под собой 2 тупеньких предположения
1) Фичи независимы
	- В то время, как слова обычно связаны - очевидно где sun, там чаще hot чем cold, и там чаще пляж или пустыня, чем тайга
2) Связанная частота в корпусе (???)
	- Предполагает, что распределение классовы в выборке(на которой мы учимся) и в итоговой Генеральной Совокупности одинаковы, что часто неправда в реальном мире

![[Pasted image 20230830084737.png]]
![[Pasted image 20230830084919.png]]![[Pasted image 20230830084930.png]]![[Pasted image 20230830085055.png]]

## Some errors and methods to fix them
Ошибки часто могут возникать в НЛП. Слова могут потерять смысл при изменении пунктуации и удалении стоп слов (препроцесинг)
- П

![[Pasted image 20230830085421.png]]![[Pasted image 20230830085454.png]]![[Pasted image 20230830085514.png]]![[Pasted image 20230830085548.png]]![[Pasted image 20230830085626.png]]