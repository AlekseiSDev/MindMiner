{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_qdrant.qdrant import QdrantVectorStore, RetrievalMode\n",
    "from mistralai import Mistral\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "from app.core.embeddings import bgem3_dense, bgem3_sparse\n",
    "from app.core.settings import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: generate some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid files total: 335\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/test_vault'\n",
    "\n",
    "files = os.listdir(data_folder)\n",
    "\n",
    "text_dicts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(f'{data_folder}/{file}', 'r') as content:\n",
    "        text = content.read()\n",
    "        if len(text)>8:\n",
    "            text_dicts += [{'filename':file, 'text':text}]\n",
    "\n",
    "print(f'Valid files total: {len(text_dicts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Mistral(api_key=\"EPQd9IXHTe6bCZoa4w5mzTzTxKfnGBut\")\n",
    "\n",
    "model = \"open-mistral-nemo\"\n",
    "\n",
    "system = '''я разрабатываю систему для ответа на вопросы, мне нужно протестировать, правильный ли текст используется для получения ответа\n",
    "сгенерируй 10 вопросов по тексту ниже на русском языке и верни их в формате json {\"questions\":[\"..\"]}'''\n",
    "\n",
    "def generate_q(text: str) -> dict:\n",
    "    chat_completion = client.chat.complete(messages=[{'role':'system', 'content':system},\n",
    "                                                     {'role':'user', 'content':text}], \n",
    "                                model=model,\n",
    "                                response_format={ \"type\": \"json_object\" })\n",
    "\n",
    "    return json.loads(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question generation: 100%|██████████| 335/335 [49:49<00:00,  8.62s/it]\n"
     ]
    }
   ],
   "source": [
    "for text_dict in tqdm(text_dicts, desc='Question generation'):\n",
    "    time.sleep(5)\n",
    "    text_dict.update({\"questions\":generate_q(text_dict['text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions total: 3350\n"
     ]
    }
   ],
   "source": [
    "text_question_dicts = []\n",
    "\n",
    "for text in text_dicts:\n",
    "    text_question_dicts += [{'filename':text['filename'],\n",
    "                             'text':text['text'],\n",
    "                             'question':text['questions']['questions'][j]} for j in range(10)]\n",
    "\n",
    "print(f'Questions total: {len(text_question_dicts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To skip full generation step, uncomment code below\n",
    "# text_question_dicts = json.load(open('data/metrics_checkpoints/texts_with_questions.json', 'r'))\n",
    "# text_question_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving chunks: 100%|██████████| 3360/3360 [21:39<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "qvs = QdrantVectorStore(QdrantClient('localhost:6333'),\n",
    "    embedding=bgem3_dense,\n",
    "    vector_name=\"text-dense\",\n",
    "    sparse_embedding=bgem3_sparse,\n",
    "    sparse_vector_name=\"text-sparse\",\n",
    "    collection_name=settings.collection_name,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    ")\n",
    "retriever = qvs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "for text_question_dict in tqdm(text_question_dicts, desc='Retrieving chunks'):\n",
    "    docs = retriever.invoke(text_question_dict['question'])\n",
    "\n",
    "    sources = [i.metadata['source'] for i in docs]\n",
    "    target = f'/content/obsiminers/{text_question_dict['filename']}'\n",
    "    rank = sources.index(target) if target in sources else -1\n",
    "\n",
    "    text_question_dict.update({'chunks':sources, \n",
    "                               'rank':rank}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To skip full retrieval step, uncomment code below\n",
    "# text_question_dicts = json.load(open('data/metrics_checkpoints/texts_with_retrieved_chunks.json', 'r'))\n",
    "# text_question_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: calculate recall@k, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@1': 0.5782089552238806,\n",
       " 'recall@1': 0.5782089552238806,\n",
       " 'mrr@3': 0.6520398009950248,\n",
       " 'recall@3': 0.7435820895522388,\n",
       " 'mrr@5': 0.663233830845771,\n",
       " 'recall@5': 0.7919402985074627,\n",
       " 'mrr@10': 0.6704295190713101,\n",
       " 'recall@10': 0.8450746268656717}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(text_question_dicts)\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for k in [1,3,5,10]:\n",
    "    metrics[f'mrr@{k}'] = df['rank'].apply(lambda x: 1/(x+1) if x>=0 and x<k else 0).mean()\n",
    "    metrics[f'recall@{k}'] = df[df['rank'].between(0,k-1)].shape[0]/df.shape[0]\n",
    "\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
